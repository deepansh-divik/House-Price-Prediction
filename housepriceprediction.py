# -*- coding: utf-8 -*-
"""HousePricePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qI0YcjBDKtHm4x2-cTAo6fFCr37D4qhD
"""

import pandas as pd

housing  = pd.read_csv("data.csv")

housing.head()

housing.info()

housing['CHAS'].value_counts()

housing.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# for creating histogram
# import matplotlib.pyplot as plt
# housing.hist(bins=50,figsize=(20,20))

"""# **Training-Test Data Splitting**"""

# creating a function that splits test and training sets
# same working as internal working of test_train_split of sklearn.model_selection
import numpy as np
def split_train_test(data,test_ratio):
  np.random.seed(42)
  shuffled = np.random.permutation(len(data))
  test_set_size = int(len(data)*test_ratio)
  test_indices = shuffled[:test_set_size]
  train_indices = shuffled[test_set_size:]
  return data.iloc[train_indices], data.iloc[test_indices]
# not used in our code, we have used sklearn's train-test-split instead

# train_set, test_set = split_train_test(housing,0.2)
# print(len(train_set))
# print(len(test_set))

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

print(len(train_set))
print(len(test_set))

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['CHAS']):
  strat_train_set = housing.loc[train_index]
  strat_test_set = housing.loc[test_index]

# strat_test_set['CHAS'].value_counts()

# strat_train_set['CHAS'].value_counts()

housing = strat_train_set.copy()

"""# **Looking For Correlations**"""

corr_matrix = housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix
attributes = ["MEDV","RM","ZN","LSTAT"]
scatter_matrix(housing[attributes], figsize= (10,8))

"""# **Trying Out Attribute Combinations**"""

housing["TAXRM"] = housing["TAX"]/housing["RM"]

housing.head()

corr_matrix = housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)

housing.plot(kind="scatter", x="TAXRM", y="MEDV", alpha=0.8, figsize=(5,3))

housing = strat_train_set.drop("MEDV", axis=1);
housing_labels = strat_train_set["MEDV"].copy()

"""# **Missing Data**

to take care of missing data, we can do these
1. drop missing data points
2. drop whole attribute having missing data
3. set value to something(0 or mean or mode or median)
"""

# 1st option
housing.dropna(subset = ["RM"]).shape
# rows with missing data are deleted
# our original dataset will remain unchanged

# 2nd option
housing.drop("RM", axis=1).shape
# RM column is deleted
# our original dataset will remain uncha

#3rd option, filling median
median = housing["RM"].median()
housing["RM"].fillna(median)
# our original dataset will remain unchanged

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy = "median")
imputer.fit(housing)

imputer.statistics_

X = imputer.transform(housing)

housing_tr = pd.DataFrame(X, columns=housing.columns)

housing_tr.describe()

"""# **Scikit-learn Design**

Primarily three types of objects

1. Estimators - It estimates some parameter based on a dataset. Ex. Imputer.
   It has a fit and transform method.
   fit() method - fits the dataset and calculates internal parameters

2. Transformers - It takes input and returns output based on the learnings from fit(). It also has a convinience function called fit_transform() which fits then transforms.

3. Predictors - LinearRegression is an example of this. fit() and predict() are two common functions. It also gives score() function which evaluates the predictions.

# **Feature Scaling**

Primarily, two types of feature scaling
1.  Normalization (Min-Max method)

    (value - min) / (max - min)

    sklearn provides a class called MinMaxScaler for this
2.Standardization

  (value - mean) / (standard-deviation)

  sklearn provides a class called StandardScaler for this

# **Creating a Pipeline**
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
my_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler())
])

housing_num_tr = my_pipeline.fit_transform(housing)

housing_num_tr

"""# **Selecting a desired model**"""

from sklearn.linear_model import LinearRegression #giving huge error
from sklearn.tree import DecisionTreeRegressor #overfitting
from sklearn.ensemble import RandomForestRegressor
# model = LinearRegression()
# model = DecisionTreeRegressor()
model = RandomForestRegressor()
model.fit(housing_num_tr, housing_labels)

some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]

prepared_data = my_pipeline.transform(some_data)

model.predict(prepared_data)

list(some_labels)

"""# **Evaluating the model**"""

from sklearn.metrics import mean_squared_error
housing_predictions = model.predict(housing_num_tr)
mse = mean_squared_error(housing_labels, housing_predictions)
rmse = np.sqrt(mse)

rmse

"""# **Using Better Evaluation Technique - Cross Validation**"""

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, housing_num_tr, housing_labels, scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)

rmse_scores

def print_scores(scores):
  print("scores: ", scores)
  print("mean: ", scores.mean())
  print("standard deviation: ", scores.std())

print_scores(rmse_scores)

"""# **Saving the Model**"""

from joblib import dump, load
dump(model, 'HPP.joblib')

# downloading HPP.joblib to local machine
# from google.colab import files
# files.download('HPP.joblib')

"""# **Testing the Model on Test Data**"""

X_test = strat_test_set.drop("MEDV", axis=1)
Y_test = strat_test_set["MEDV"]
x_prepared = my_pipeline.transform(X_test)
final_predictions = model.predict(x_prepared)
final_mse = mean_squared_error(Y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

final_rmse

"""# **Using the Model**"""

model = load('HPP.joblib')

features = [[-0.46942006,  3.12628155, -1.17165014, -0.27288841, -1.42262747,
        -0.24626574, -1.31238772,  2.61111401, -1.0016859 , -0.5778192 ,
        -0.97491834,  0.41164221, -0.86691034]]
model.predict(features)